


Day 1
- Python & bash basics
- ffmpeg
- feature extraction & viz with LibRosa
- onset detection with Aubio
- messing with files in PyDub
- creating corpora: scraping mp3s & rtmp streams

Day 2
- Sonic Visualiser
- spectrograms and the nature of speech
- pyAudioAnalysis
- finding NBC chimes and applause
- tagging their own corpora

Day 3
- neural networks
- GMMs
- speech to text
- classifying their own corpora
- using VPSes

Day 4
- refining classification projects
- evaluating results
- smoothing and optimizing
- sharing data







####


Checking if a file exists

!du /home/sharedfolder/cpb-aacip-15-p26pz51t8d**CBSNS730426X*.h264.wav
## Basic SmacPy classifier



```
for f in *.wav; do
ffmpeg -f u16le -ar 44100 -ac 1 -i $f 44s/$f ;
done

from smacpy import Smacpy

model = Smacpy("wavs/training", {'/Users/mclaugh/Dropbox/NBC*Chime*Search/chimes/CBD-440606*NBC0300-NewsandMusic*v2.wav':'chime', '/Users/mclaugh/Dropbox/NBC*Chime*Search/chimes/CBD-440606*NBC0700-News*Marker 01 Copy*v2.wav':'chime', '/Users/mclaugh/Dropbox/NBC*Chime*Search/chimes/CBD-440606*NBC1515-MusicandNews*v2.wav':'chime', '/Users/mclaugh/Dropbox/NBC*Chime*Search/chimes/CBD-440607*NBC0845-ModernRomances*v2.wav':'chime','/Users/mclaugh/Dropbox/NBC*Chime*Search/notchimes/CBD-440607*NBC1030-Helpmate*start*398.0*dur*3.0*class*0.0.wav':'not*chime', '/Users/mclaugh/Dropbox/NBC*Chime*Search/notchimes/CBD-440607*NBC1030-Helpmate*start*642.87*dur*3.0*class*0.0.wav':'not*chime', '/Users/mclaugh/Dropbox/NBC*Chime*Search/notchimes/CBD-440607*NBC1145-DavidHarum*newsinterruption*start*26.1*dur*3.0*class*0.0.wav':'not*chime', '/Users/mclaugh/Dropbox/NBC*Chime*Search/notchimes/CBD-440607*NBC1145-DavidHarum*newsinterruption*start*372.65*dur*3.0*class*0.0.wav':'not*chime'})

model.classify('wavs/testing/hubert01.wav')
```
## Create Spectrograms on CLI

```bash
for f in *.mp3; do
basename=`basename $f .mp3`
ffmpeg -i """$f""" -f segment -segment*time 20 """$f""".%04d.wav ;
done
```


```bash
for f in *.wav; do
basename=`basename $f .wav`
sox $f -n spectrogram -x 1600 -y 513 -r -o $basename.png;
done
```



## ASIST

- General introduction:
- Q: What does it mean to set up a good machine learning problem?
- How to pick a productive query for audio
- the kinds of sounds you might you look for

- Executing a project:

1. build corpus
2. annotating corpus
3. Designing a ML query
4. Running query
5. Validating



## My rough outline for the ASIST workshop:

#### Overview

- Introductions among crowd & brief HiPSTAS/ARLO background
- What's the problem you're trying to solve?
- What are you searching for?
- What don't you know about what's in your archive?

- Point: We're coming from the humanities and we want to be able to use these tools ourselves.

- Get them in the audio mindset
- Talk about why sound is interesting and why it's difficult to work with
- good spot to pass around 3D-printed spectral data ()
- This isn't automatic transcription; it's messy and opaque compared to working with text.
- This discussion includes audio quality: recordings on street, cassette dubs, etc.
- Don't trust the performance measures you see in machine learning papers; they're typically using internally consistent corpora.
- Stress: What makes a particular sound identifiable?
- Teaching an ML algorithm is like training a child.

#### Tools and Standards

- Talk about other software: 
- Open source tools: Praat, Sonic Visualizer, ARLO, FFmpeg, Pydub, Aubio, etc.

- Talk about Marit's article.
- Her NEH grant may be helpful — lots of details on audio tools
- Hartwell Francis et al. article on sound in libraries and archives


- Slide with various software tools and how they connect
- Talk about Adobe Audition vs. Audacity and what role they might play in your workflow.
- Use free software when possible

- Identify practical ways to design, organize, and present information about an audio collection

- Discuss how datasets extracted from audio collections may be used in accordance with existing practices for metadata standards, teaching, and research. 

#### Interactive Demo

- Walk them through a workflow and introduce software along the way.

- Things we want to cover:
- examine audio
- isolate phenomena to search for
- annotate
- assemble data and 
- run classification
- demonstrate the rigor of your approach 

- i.e., from tags to CSV to ARLO to CSV + visualization/stats software

- Evaluation methods
- cross-validation
- sampling-based validation on an unknown audio collection
- Visualization, smoothing
- Determining optimal classification threshold and window function for machine learning data
- Don't put too much stock in your accuracy score — an imperfect measure for many classification tasks.
- Analyzing measurements across a collection
- When data is noisy, correcting classification instances by hand for analysis in aggregate



## And here's a rough outline I included in our first email about the workshop:

1. the basics of working with audio data (as opposed to text, for instance)
2. an ARLO interface walkthrough
3. explanation of the IBL algorithm vs. SVM and other ML approaches
4. the process involved in creating training sets
5. picking initial comparison parameters
6. overview of specs and configuration options for ARLO's back end
7. optimizing the accuracy of output data
8. cleaning the optimized data
9. running statistical tests between groups
10. interpreting and qualifying final results

## HILT Early Outline

Q: What does it mean to set up a good machine learning problem?





1:30–5 PM


1:30–2:00
- People introductions & brief HiPSTAS/ARLO introduction
- What's your problem?

- Point: We're coming from the humanities and we want to be able to use these tools


Get them in the mindset of sounds:

- Talk about why sound is interesting — its characteristics
- This isn't automatic transcription; it's hard to work with
- stress: sounds that are identifaable
- Teaching an ML algorithm is like training a child ...
- This discussion includes audio quality: recordings on street, cassette dubs, etc.




2:15–30
- Talk about other software: 
- Open source tools:
Praat, Sonic Visualizer, ARLO, and the command line programs Aubio and FFmpeg.
- Talk about Adobe Audition


- Talk about Marit's article
- Her NEH grant may be helpful — lots of details on audio tools
- and Hartwell Francis article on sound in libraries and archives



•   Identify practical ways to design, organize, and present information about an audio collection
•   Use free software tools to process and extract data from audio files
•   Use scripting to determine optimal classification threshold and window function for machine learning data
Discuss how datasets extracted from audio collections may be used in accordance with existing practices for metadata standards, teaching, and research. 




----
Tanya: 
- Walk them through a workflow
and introduce software along the way


These are the ~5 things we want to do:

pull up audio
isolate
annotate
collect them together

Maybe get people involved: What do you think  ...

These tools do these different parts of the workflow:
slide with various software tools: Audition, etc. ... linked together in a workflow.

From tags to CSV to ARLO


Talk about metadata too
- Tanya wants to talk about this: integrating into existing metadata schemas


Evaluation methods
cross-validation, maybe larger validation

visualization, smoothing



Discussion:








-----
-spitballing email:


I agree that just showing off ARLO's potential might not make a great session. It would be great if people could do something with it, especially considering we have four hours to fill. But if ARLO isn't live, I could talk in detail about each step in our workflow and encourage discussion along the way. That might look something like this:

1. the basics of working with audio data (as opposed to text, for instance)
2. an ARLO interface walkthrough
3. explanation of the IBL algorithm vs. SVM and other ML approaches
4. the process involved in creating training sets
5. picking initial comparison parameters
6. overview of specs and configuration options for ARLO's back end
7. optimizing the accuracy of output data
8. cleaning the optimized data
9. running statistical tests between groups
10. interpreting and qualifying final results

Etc., etc. I could probably fill half a day, though the format above might be a bit dry.

Just spitballing: Even if we were ready to let people test drive ARLO on our server, it might take days or weeks to get results back. The only alternative I can think of is to install ARLO on an AWS server with 100 or so virtual cores (pretty cheap) and run it with a smallish dataset. If my envelope math is correct, with 100 or 200 training examples (instead of 5000 for the applause job) and 200 hours of audio (instead of 6000 for PennSound), we could probably get classification data back in around an hour. I know ARLO is set up for parallel processing, though I'm not sure how much work it would be to scale it to that many cores.

The real limiting factor is Tony's time, of course. If you think this kind of setup is remotely feasible, I'll write him and maybe set up a phone call to see what he thinks. If not, I can provide sample data and focus more on optimization and interpretation. Or if you think this is a terrible idea, we can just cancel it.

>I also want to make a weekly, scheduled meeting with you throughout the semester. At this point, my preference would be Wednesday afternoons. I'd like to make it a big block so that we can talk about your work, talk about the project, do some work, etc. I'm thinking something like 1:30-4:30. Does that work for you? Obviously, this counts as part of your 20 hours.

Meeting weekly on Wednesday sounds good. The fall semester begins 8/24, which is a Wednesday. Let's say 1:30–4:30 and you can let me know if another time looks better.
Wget 

Use applause


Nbc 


2
YouTube-dl 
Pandas


Hilt

Make a podcast ad remover






Survey
Os 
Laptop

Evaluate how well pyAudioAnalysis emotion detector does

First task: install ffmpeg properly





Give them audio related challenges to look up on stack overflow

- reverse audio from command line (backmasking example)
- pitch something up
- 



Use onset detection to compare the speed of two speakers


Create pitch traces and represent pitch ranges as histogram
  - show difference between speech and performance by same speaker?




Which comedian gets more laughter?




Search 78 recordings for ...
 - yodeling
 - accordion?
 - 




Use an API to build corporate based on search terms ....



Adapting dejavu audio fingerprinting tool to accept audio file ...



Use pocketsphinx -- have people search for X topic within a corpus






Pulling metadata from mp3s and managing in a csv




The sunny D laugh finder

Search for dj drops?






Assignments by eye:
 - find the transcoded or cassette-sourced recording in a set of 10
 -  find the explosion in a movie clip ...




Assignment: teach them music/speech, then have them write subpricess code to extract all non-music to a folder




With dejavu -- find X song used in Y movie






Point for ppt: I want to help you on your interests





 


Audio thumbnailing



















- get some USB drives/hard drives ...








HILT Planning Notes

Q: What does it mean to set up a good machine learning problem?





1:30–5 PM


1:30–2:00
- People introductions & brief HiPSTAS/ARLO introduction
- What's your problem?

- Point: We're coming from the humanities and we want to be able to use these tools


Get them in the mindset of sounds:

- Talk about why sound is interesting — its characteristics
- This isn't automatic transcription; it's hard to work with
- stress: sounds that are identifaable
- Teaching an ML algorithm is like training a child ...
- This discussion includes audio quality: recordings on street, cassette dubs, etc.




2:15–30
- Talk about other software:
- Open source tools:
Praat, Sonic Visualizer, ARLO, and the command line programs Aubio and FFmpeg.
- Talk about Adobe Audition


- Talk about Marit's article
- Her NEH grant may be helpful — lots of details on audio tools
- and Hartwell Francis article on sound in libraries and archives



•   Identify practical ways to design, organize, and present information about an audio collection
•   Use free software tools to process and extract data from audio files
•   Use scripting to determine optimal classification threshold and window function for machine learning data
Discuss how datasets extracted from audio collections may be used in accordance with existing practices for metadata standards, teaching, and research.




----
Tanya:
- Walk them through a workflow
and introduce software along the way


These are the ~5 things we want to do:

pull up audio
isolate
annotate
collect them together

Maybe get people involved: What do you think  ...

These tools do these different parts of the workflow:
slide with various software tools: Audition, etc. ... linked together in a workflow.

From tags to CSV to ARLO


Talk about metadata too
- Tanya wants to talk about this: integrating into existing metadata schemas


Evaluation methods
cross-validation, maybe larger validation

visualization, smoothing



Discussion:








-----
-spitballing email:


I agree that just showing off ARLO's potential might not make a great session. It would be great if people could do something with it, especially considering we have four hours to fill. But if ARLO isn't live, I could talk in detail about each step in our workflow and encourage discussion along the way. That might look something like this:

1. the basics of working with audio data (as opposed to text, for instance)
2. an ARLO interface walkthrough
3. explanation of the IBL algorithm vs. SVM and other ML approaches
4. the process involved in creating training sets
5. picking initial comparison parameters
6. overview of specs and configuration options for ARLO's back end
7. optimizing the accuracy of output data
8. cleaning the optimized data
9. running statistical tests between groups
10. interpreting and qualifying final results

Etc., etc. I could probably fill half a day, though the format above might be a bit dry.

Just spitballing: Even if we were ready to let people test drive ARLO on our server, it might take days or weeks to get results back. The only alternative I can think of is to install ARLO on an AWS server with 100 or so virtual cores (pretty cheap) and run it with a smallish dataset. If my envelope math is correct, with 100 or 200 training examples (instead of 5000 for the applause job) and 200 hours of audio (instead of 6000 for PennSound), we could probably get classification data back in around an hour. I know ARLO is set up for parallel processing, though I'm not sure how much work it would be to scale it to that many cores.

The real limiting factor is Tony's time, of course. If you think this kind of setup is remotely feasible, I'll write him and maybe set up a phone call to see what he thinks. If not, I can provide sample data and focus more on optimization and interpretation. Or if you think this is a terrible idea, we can just cancel it.

>I also want to make a weekly, scheduled meeting with you throughout the semester. At this point, my preference would be Wednesday afternoons. I'd like to make it a big block so that we can talk about your work, talk about the project, do some work, etc. I'm thinking something like 1:30-4:30. Does that work for you? Obviously, this counts as part of your 20 hours.

Meeting weekly on Wednesday sounds good. The fall semester begins 8/24, which is a Wednesday. Let's say 1:30–4:30 and you can let me know if another time looks better.
